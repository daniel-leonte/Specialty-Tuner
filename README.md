# SpecialtyTuner ‚Äì Compact ML Code Generation Assistant

SpecialtyTuner is a lightweight *instruction-tuned* language model that writes **production-ready Python code for common machine-learning and data-science tasks**.  
The project shows how far you can push a modern open-source LLM by combining:

* CodeLlama-7b-hf as a base model (‚âà7 B parameters)
* Parameter-efficient **LoRA** fine-tuning (rank = 16) on hand-curated ML examples
* 4-bit / 8-bit quantisation for memory-friendly inference

---

## ‚ú® Key Features

|      | What you get |
|------|--------------|
| ‚ö° **Fast offline inference** | Quantised weights + small LoRA adapter (‚âà30 MB) |
| ü™Ñ **One-shot code generation** | CLI returns a fully formatted `black`-compliant script for your prompt (`pandas`, `scikit-learn`, `torch`, ‚Ä¶) |
| üîç **Quality gates** | Built-in evaluation script checks syntax, execution and *pass@k* unit tests |
| üõ† **End-to-end pipeline** | `collect_data ‚Üí clean_data ‚Üí train ‚Üí eval ‚Üí infer` ‚Äì all reproducible with plain Python |
| üì¶ **Self-contained repo** | No hidden services ‚Äì everything runs locally with open-source libraries |
| üß∞ **Modular architecture** | Shared utilities module for consistent device handling, code extraction and processing |

---

## Quick Start

```bash
# 1. Clone & install
$ git clone <your-repo-url> specialtytuner && cd specialtytuner
$ python3 -m venv .venv && source .venv/bin/activate
$ pip install -r requirements.txt

# 2. (Optional) download the released LoRA checkpoint
$ wget -O checkpoints/best/adapter_model.bin <link-to-checkpoint>

# 3. Generate code ‚ú®
$ python scripts/infer.py \
        --prompt "Load a CSV with pandas, drop NA rows, then train a RandomForestClassifier" \
        --temperature 0.2 --top_p 0.95 --streaming
```

The assistant prints a fully formatted script enclosed in markdown fences:

```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
# ...
```

---

## Repository Layout

```
.
‚îú‚îÄ‚îÄ data/              # train / val / test JSONL after cleaning
‚îú‚îÄ‚îÄ scripts/           # all pipeline steps
‚îÇ   ‚îú‚îÄ‚îÄ collect_data.py   # fetch & merge open datasets
‚îÇ   ‚îú‚îÄ‚îÄ clean_data.py     # format, lint, quality-score
‚îÇ   ‚îú‚îÄ‚îÄ train.py          # LoRA fine-tuning
‚îÇ   ‚îú‚îÄ‚îÄ eval.py           # syntax / exec / unit-test evaluation
‚îÇ   ‚îú‚îÄ‚îÄ infer.py          # CLI inference
‚îÇ   ‚îú‚îÄ‚îÄ generate_demo.py  # build nice markdown demo gallery
‚îÇ   ‚îî‚îÄ‚îÄ utils.py          # shared utilities for device detection and code handling
‚îú‚îÄ‚îÄ checkpoints/       # tiny LoRA adapter (generated after training)
‚îú‚îÄ‚îÄ demo_outputs.md    # example generations
‚îú‚îÄ‚îÄ requirements.txt   # python deps
‚îî‚îÄ‚îÄ README.md          # you are here
```

---

## üîß Training Pipeline

1. **Collect** open-source tasks (DS-1000, HumanEval, CodeAlpaca subset):
   ```bash
   python scripts/collect_data.py
   ```
2. **Clean & score** the raw snippets ‚Üí `train.jsonl / val.jsonl / test.jsonl`:
   ```bash
   python scripts/clean_data.py
   ```
3. **LoRA Fine-tune** on 7-B base model (4-bit CUDA or 8-bit CPU):
   ```bash
   python scripts/train.py
   ```
4. **Evaluate** on held-out tasks (20 examples by default):
   ```bash
   python scripts/eval.py
   ```

---

## üìä Current Results (20 hold-out prompts)

| Metric | Score |
|--------|-------|
| Syntax correctness | **95 %** |
| Execution success  | 5 % |
| pass@5 unit tests  | 0 % |

> Tip: Most failures come from missing runtime dependencies ‚Äì improving the import heuristic is next on the roadmap.

---

## In-depth Usage

### Inference flags

| Flag | Default | Purpose |
|------|---------|---------|
| `--temperature` | 0.2 | Sampling temperature |
| `--top_p`       | 0.95 | Nucleus sampling cut-off |
| `--max_length`  | 800  | Max tokens in generation |
| `--streaming`   | off  | Stream tokens as they arrive |

### Generate a demo gallery

```bash
python scripts/generate_demo.py --output_file demo_outputs.md
```

### Run evaluation on more examples

```bash
python scripts/eval.py   # uses data/test.jsonl
```

---

## Code Architecture

The codebase follows software engineering best practices:

* **DRY Principle**: Common functionality extracted to `utils.py` to eliminate code duplication
* **Hardware Adaptability**: Automatic detection and configuration for CUDA, MPS (Apple Silicon), or CPU environments
* **Consistent Formatting**: Standardized code formatting via Black
* **Error Handling**: Robust error handling throughout the pipeline
* **Documentation**: Clear docstrings with type hints for better maintainability

---

## Limitations & Future Work

* The base model has no internet or file-system awareness; paths and URLs are placeholders.
* Execution success is still low ‚Äì integrating an auto-fix loop is planned.
* Only single-file scripts are produced; multi-module projects are out-of-scope for now.
* The dataset is relatively small (~600 curated examples) ‚Äì scaling the corpus should unlock further quality gains.

---

## License

This repository is released under the MIT License.  
The fine-tuned weights inherit the original CodeLlama licence.

---

## Acknowledgements

* [CodeLlama](https://github.com/facebookresearch/codellama) for the stellar base model
* HuggingFace ü§ó ‚Äì *transformers*, *datasets*, *peft*, *accelerate*
* [DS-1000](https://huggingface.co/datasets/xlangai/DS-1000), HumanEval & CodeAlpaca datasets
* The open-source community for tools like *black*, *bitsandbytes* and *pytest*